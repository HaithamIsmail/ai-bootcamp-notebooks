{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron\n",
    "It is an unusual-looking cell mostly found in animal brains. It’s composed of a cell body containing the nucleus and most of the cell’s complex components, many branching extensions called dendrites, plus one very long extension called the axon. The axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called telodendria, and at the tip of these branches are minuscule structures called synaptic terminals (or simply synapses), which are connected to the dendrites or cell bodies of other neurons. Biological neurons produce short electrical impulses called action potentials (simply signals) which travel along the axons and make the synapses release chemical signals called neurotransmitters. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses\n",
    "\n",
    "<img src='images\\neurons.png' height='200px'>\n",
    "<img src='images\\bnn.png' height='200px'>\n",
    "\n",
    "## Artificial Neurons\n",
    "\n",
    "Artificial networks are inspired by the human brain and how it is interconnected. They both have neurons, activations, and large interconnectivity. However, it is not a perfect comparison as the underlying process is different.\n",
    "\n",
    "\n",
    "<img src='images\\bio vs ai.png' height='300px'>\n",
    "\n",
    "This is similar to what we saw in logistic regression, the only difference is that in logsitic regresssion what we call the activation function was a sigmoid. In neural networks, there is a wide variety of activation functions to choose from. We will discuss activation function later in this lesson. \n",
    "\n",
    "Within a neural network, each neuron performs a computation akin to logistic regression. It takes input from the previous layer, applies weights to these inputs, sums them up, and then applies an activation function. This process resembles the calculation in logistic regression where features are weighted and summed before being passed through a sigmoid activation function.\n",
    "\n",
    "A shallow neural network would have only 2 layers, one hidden and one output layer. \n",
    "\n",
    "<img src='images\\shallow network.png' height='300px'>\n",
    "\n",
    "Adding more hidden layers will make it a deep neural network.\n",
    "\n",
    "<img src='images\\2hidden.jpg' height='300px'>\n",
    "\n",
    "\n",
    "**- Learning Complex Patterns:** By linking multiple neurons together in layers, neural networks can capture intricate patterns and relationships in the data. While logistic regression is limited to linear relationships, neural networks excel at modeling nonlinearities due to their layered structure and the activation functions applied at each neuron.\n",
    "\n",
    "**- Scaling Complexity:** Just as complex structures can be built from simple building blocks, neural networks scale the capabilities of logistic regression by layering multiple logistic regression units. Each layer adds another level of abstraction, allowing the network to learn increasingly complex representations of the data.\n",
    "\n",
    "### Anatomy of a Neuron:\n",
    "\n",
    "**1. Inputs:** Neurons receive input signals from the previous layer or directly from the input data. These inputs represent the features of the dataset.\n",
    "\n",
    "**2. Weights:** Each input is associated with a weight, which determines its importance in the computation. Just like logistic regression assigns coefficients to features, neural networks adjust weights during training to optimize performance.\n",
    "\n",
    "**3. Activation Function:** After summing the weighted inputs, the neuron applies an activation function. This function introduces nonlinearity into the model, allowing neural networks to learn complex mappings between inputs and outputs. Common activation functions include the sigmoid function, which is used in logistic regression, as well as others like Softmax, ReLU (Rectified Linear Unit) and tanh.\n",
    "\n",
    "**3. Output:** Finally, the neuron produces an output value, which is transmitted to the next layer of neurons. In classification tasks, this output typically represents the probability of a certain class, just as logistic regression outputs probabilities for binary classification problems.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "$$ f_i^{[l]}(x) = w_{i, 1}^{[l]}.f_1^{[l-1]}(x)+ w_{i, 2}^{[l]}.f_2^{[l-1]}(x) + b_1^{[l]} $$\n",
    "\n",
    "- The superscript $[l]$ indicates the layer\n",
    "- The subscript $i, j$, $\"i\"$ indicates the neuron number, $\"j\"$ indicates the coeffiecent number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Activation functions play a vital role in neural networks by transforming the input signal of a node into an output signal, which is then forwarded to the next layer. They enables neural networks to learn intricates patterns in data, breaking away from solely linear relationships. By introducing non-linearities, activation functions empower neural networks to capture and understand complex mappings between inputs and outputs. Without them, the network's capacity to learn would be limited.\n",
    "\n",
    "So why do we need it in the first place? A neural network with layers only having linear activations will be the same as having a single linear layer.\n",
    "\n",
    "<img src='images\\linear activation.png' height='200px'>\n",
    "\n",
    "$$\n",
    "\n",
    "Layer 1, neuron 1: f_1^{[1]}(x) = w_{11}^{[1]}.x + b_1^{[1]}\n",
    "\n",
    "\\\\[10pt]\n",
    "\n",
    "Layer 1, neuron 2: f_2^{[1]}(x) = w_{21}^{[1]}.x + b_2^{[1]}\n",
    "\n",
    "\\\\[20pt]\n",
    "\n",
    "Layer 2, neuron 1: f_1^{[2]}(x) = w_{11}^{[2]}.f_1^{[1]}(x)+ w_{12}^{[2]}.f_2^{[1]}(x) + b_1^{[2]}\n",
    "\n",
    "\\\\[10pt] \n",
    "\n",
    "f_1^{[2]}(x) = w_{11}^{[2]}.(w_{11}^{[1]}.x + b_1^{[1]})+ w_{12}^{[2]}.(w_{21}^{[1]}.x + b_2^{[1]}) + b_1^{[2]}\n",
    "\n",
    "\\\\[10pt] \n",
    "\n",
    "f_1^{[2]}(x) = (w_{11}^{[2]}w_{11}^{[1]} +  w_{12}^{[2]}w_{21}^{[1]})x + w_{11}^{[2]}b_1^{[1]} + w_{12}^{[2]}b_2^{[1]} + b_1^{[2]}\n",
    "\n",
    "\\\\[10pt] \n",
    "\n",
    "f_1^{[2]}(x) = w.x + b\n",
    "\n",
    "$$\n",
    "\n",
    "We are back to a linear model... That's why we need activations to fit more complex relations. Here is what a neural network, with two hidden layers and only linear activations, would fit for a sine function\n",
    "\n",
    "A (1 - input) (8 neurons, 1st hidden layer) (8 neurons, 2nd hidden layer) (1 neuron, output layer)\n",
    "\n",
    "<img src='images\\1881 nn.png' height='300px'>\n",
    "\n",
    "With only linear activations, here is how it will fit a sine function:\n",
    "\n",
    "<img src='images\\1881 linear activation nn.png' height='200px'>\n",
    "\n",
    "Where as this is the result when we use a ***ReLU*** activation function:\n",
    "\n",
    "<img src='images\\relu function.png' height='200px'> &nbsp; <img src='images\\1881 relu activation nn.png' height='200px'>\n",
    "\n",
    "\n",
    "let's see how using the ***ReLU*** activation function can help a neural network learn a sine function:\n",
    "\n",
    "<video controls src=\"videos\\how activations work.mp4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "From what we already saw, we can already tell that a single layer neural network is just like logistic regression, so we can simply use gradient descent to update the weights and biases, and it is straight forward to implement. However, it is more much more complicated for deeper networks. It was complicated enough that it took about 30 years before researchers figured out how to do it.\n",
    "\n",
    "For many years researchers struggled to find a way to train neural networkss without success. But in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a groundbreaking paper that introduced the backpropagation training algorithm, which is still used today.\n",
    "\n",
    "In short, it is Gradient Descent using an efficient technique for computing the gradients automatically in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. \n",
    "\n",
    "In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
    "\n",
    "<img src='images\\backprop_diagram.png' height='500px'>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.\n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial \\ell}{\\partial L_2} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_1}{\\partial W_1} \n",
    "$$\n",
    "\n",
    "\n",
    "No need to sweat over the details, this knowledge requires some knowledge in vector calculus. What we are doing is finding a way to approximate the gradients for each parameter so we can use gradient descent over it.\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow\n",
    "\n",
    "To build neural networks we will be using a library called TensorFlow. TensorFlow is an open source and an end to end platform used for building machine learning models. Being end to end, you can prepare data, build models, diagnose, improve, and deploy them.\n",
    "\n",
    "TensorFlow uses Keras at its backend. Keras is a well beautifully designed API for building deep learning models in popular fields such as Computer Vision and Natural Language Processing.\n",
    "\n",
    "TensorFlow has got a strong community, from users, learning resources and whole range of technical supports. Not only it powers majority of Google apps such as YouTube, Maps and Google Photos, it is also widely used across startups and other big techs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Tensors\n",
    "\n",
    "TensorFlow uses arrays called tensors. A tensor is a multidimensional array of the same data type. A tensor can be a scalar (single number), a vector, or a matrix.\n",
    "\n",
    "Tensors are like NumPy arrays, except that tensors have GPU(Graphical Processing Unit) support.\n",
    "\n",
    "A typical tensor has the following information:\n",
    "\n",
    "- Shape: The length or number of elements of each of the tensor dimension/axes.\n",
    "- Rank: The number of dimensions/axes in a tensor. A scalar tensor (a single number) has rank 0, a vector has a rank 1 (a vector is a 1D), and a matrix has rank 2 (or 2D).\n",
    "- Axis/Dimension: This is a particular dimension of a tensor\n",
    "- Size: This is the total number of items in the tensor.\n",
    "  \n",
    "But why use tensor/NumPy arrays?\n",
    "\n",
    "Well, almost all types of data can be represented as an array of numbers. Take an example:\n",
    "\n",
    "- An Image can be represented as an array of pixels.\n",
    "- Any text data can be converted into an array of numbers (or tokens representing words)\n",
    "- Video (made of sequence of images) can be represented as an array of numbers.\n",
    "  \n",
    "Having the ability to convert these raw data into tensors/arrays make it easy to preprocess it, either when performing conventional numerical computations or when it is the data we are preparing to feed to a machine learning model. Take a simple example, we can not feed a raw text to a machine learning model. That text has to be converted into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Tensors\n",
    "\n",
    "The most basic way of creating a tensor is using `tf.constant()`, this will create a constant that is immutable (cannot be changed)\n",
    "\n",
    "Here is a \"scalar\" or \"rank-0\" tensor . A scalar contains a single value, and no \"axes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "rank_0_tensor = tf.constant(4) # A scalar\n",
    "print(rank_0_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"vector\" or \"rank-1\" tensor is like a list of values. A vector has one axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's make this a float tensor.\n",
    "rank_1_tensor = tf.constant([2.0, 3.0, 4.0])\n",
    "print(rank_1_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"matrix\" or \"rank-2\" tensor has two axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(3, 2), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "# If you want to be specific, you can set the dtype (see below) at creation time\n",
    "rank_2_tensor = tf.constant([[1, 2],\n",
    "                             [3, 4],\n",
    "                             [5, 6]], dtype=tf.float16)\n",
    "print(rank_2_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color: white; color: black;\">\n",
    "<tr>\n",
    "  <th>A scalar, shape: <code>[]</code></th>\n",
    "  <th>A vector, shape: <code>[3]</code></th>\n",
    "  <th>A matrix, shape: <code>[3, 2]</code></th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"images/scalar.png\" alt=\"A scalar, the number 4\" />\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "   <img src=\"images/vector.png\" alt=\"The line with 3 sections, each one containing a number.\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img src=\"images/matrix.png\" alt=\"A 3x2 grid, with each cell containing a number.\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The tensor above has a shape `(3,2)` which means our tensor has 3 rows and 2 columns. \n",
    "\n",
    "You can also check the number of dimensions or axes of a tensor using `tensor_name.ndim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions in rank 0 tensor: 0\n",
      "Number of dimensions in rank 1 tensor: 1\n",
      "Number of dimensions in rank 2 tensor: 2\n"
     ]
    }
   ],
   "source": [
    "print('Number of dimensions in rank 0 tensor:', rank_0_tensor.ndim)\n",
    "print('Number of dimensions in rank 1 tensor:', rank_1_tensor.ndim)\n",
    "print('Number of dimensions in rank 2 tensor:', rank_2_tensor.ndim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like NumPy array, a tensor can have many dimensions. Here is an example of a tensor with 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n",
      "Number of dimensions in tensor_3d: 3\n"
     ]
    }
   ],
   "source": [
    "tensor_3d = tf.constant([\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]],])\n",
    "\n",
    "print(tensor_3d)\n",
    "print('Number of dimensions in tensor_3d:', tensor_3d.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color: white; color: black;\">\n",
    "<tr>\n",
    "  <th colspan=3>A 3-axis tensor, shape: <code>[3, 2, 5]</code></th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"images/3-axis_numpy.png\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img src=\"images/3-axis_front.png\"/>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "   <img src=\"images/3-axis_block.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be converted into NumPy array by calling `tensor_name.numpy` or `np.array(tensor_name)`. \n",
    "\n",
    "TensorFlow is well integrated with NumPy. And if not yet done, TensorFlow previously posted that they are working on getting the whole of NumPy into TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variabl Tensors\n",
    "\n",
    "A tensor created with tf.constant() is immutable, it can not be changed. Such kind of tensor can not be used as weights in neural networks because they need to be updated in backpropogation for example.\n",
    "\n",
    "With tf.Variable(), we can create tensors that can be mutable and thus can be used in things like updating the weights of neural networks like said above.\n",
    "\n",
    "Creating variable tensor is as simple as the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 2, 5) dtype=int32, numpy=\n",
      "array([[[1, 2, 3, 4, 5],\n",
      "        [6, 7, 8, 9, 8]],\n",
      "\n",
      "       [[1, 3, 5, 7, 9],\n",
      "        [2, 4, 6, 8, 1]],\n",
      "\n",
      "       [[1, 2, 3, 5, 4],\n",
      "        [3, 4, 5, 6, 7]]])>\n"
     ]
    }
   ],
   "source": [
    "var_tensor = tf.Variable([\n",
    "                         [[1,2,3,4,5],\n",
    "                         [6,7,8,9,8]],\n",
    "                         [[1,3,5,7,9],\n",
    "                         [2,4,6,8,1]],\n",
    "                         [[1,2,3,5,4],\n",
    "                         [3,4,5,6,7]], ])\n",
    "\n",
    "print(var_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be converted to NumPy array, just like tensors created with `tf.constant()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3, 4, 5],\n",
       "        [6, 7, 8, 9, 8]],\n",
       "\n",
       "       [[1, 3, 5, 7, 9],\n",
       "        [2, 4, 6, 8, 1]],\n",
       "\n",
       "       [[1, 2, 3, 5, 4],\n",
       "        [3, 4, 5, 6, 7]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting a variable tensor into NumPy array\n",
    "\n",
    "var_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient calculated by tf.GradientTape:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552416]\n",
      " [0.29263481 0.96963763]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "True Gradient:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696376 ]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "Maximum Difference: 5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the random seed so things are reproducible\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a random tensor\n",
    "x = tf.random.normal((2,2))\n",
    "\n",
    "# Calculate gradient\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x ** 2\n",
    "    \n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "# Calculate the actual gradient of y = x^2\n",
    "true_grad = 2 * x\n",
    "\n",
    "# Print the gradient calculated by tf.GradientTape\n",
    "print('Gradient calculated by tf.GradientTape:\\n', dy_dx)\n",
    "\n",
    "# Print the actual gradient of y = x^2\n",
    "print('\\nTrue Gradient:\\n', true_grad)\n",
    "\n",
    "# Print the maximum difference between true and calculated gradient\n",
    "print('\\nMaximum Difference:', np.abs(true_grad - dy_dx).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
