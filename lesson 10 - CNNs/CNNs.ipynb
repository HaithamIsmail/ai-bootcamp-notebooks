{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "\n",
    "Convolutional networks (LeCun, 1989), also known as convolutional neural networks or CNNs, are a specialized kind of neural network for processing data that has a known, grid-like topology. Examples include time-series data, which can be thought of as a 1D grid taking samples at regular time intervals, and image data, which can be thought of as a 2D grid of pixels. Convolutional networks have been tremendously successful in practical applications, and were able to achieve superhuman performance on some complex visual tasks.\n",
    "\n",
    "## Convolutional Layers\n",
    "\n",
    "The most important building block of a CNN is the convolutional layer. Neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in dense layers), but only to pixels in their receptive fields. That region in the input image is called the *local receptive field* for the hidden neuron. It's a little window on the input pixels. Each connection learns a weight. And the hidden neuron learns an overall bias as well.\n",
    "\n",
    "<figure>\n",
    "  <img src='images/local receptive field.png' style=\"background-color: white;\">\n",
    "  <figcaption>Local Receptive field</figcaption>\n",
    "</figure>\n",
    "\n",
    "In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images,which is one of the reasons why CNNs work so well for image recognition.\n",
    "\n",
    "The way convolution layers work is the following, we start at the top left corner of the input layer, then apply the convolution operation between the input layer and the filter (also called kernel), the result will be the value that reaches the neuron at the top left corner. Next, we will slide the window (local receptive field) and do the same for the new field to calculate the input to the 2nd neuron. When the row ends we slide the windo downwards and so on...\n",
    "\n",
    "**1. Step one: start from top left**\n",
    "\n",
    "<img src=\"images/conv p1.png\" height=\"300px\">\n",
    "\n",
    "**2. Step two: slide to the right**\n",
    "\n",
    "<img src=\"images/conv p2.png\" height=\"300px\">\n",
    "\n",
    "**3. Continue:**\n",
    "\n",
    "<img src=\"images/convolutions example.png\">\n",
    "\n",
    "The filters are the trainable parameters of the convolution layers, and they are shared by all the neurons. This is one of the most important features of the convlution networks, so let's look at what distinguishes them.\n",
    "\n",
    "### Why are convolutional networks effective\n",
    "\n",
    "**1. Parameter sharing:** \n",
    "\n",
    "CNNs utilize parameter sharing through convolutional filters, significantly reducing the number of parameters compared to fully connected networks. This makes CNNs more computationally efficient and reduces the risk of overfitting, especially when dealing with large input data like images. In simple words, we're going to use the same weights and bias for each of the hidden neuron.\n",
    "\n",
    "<img src=\"images/weights.png\" height=\"200px\">\n",
    "\n",
    "**2. Locality:**\n",
    "\n",
    "One of the most common types of structure in data is \"locality\" -- the most relevant information for understanding or predicting a pixel is a small number of pixels around it.\n",
    "\n",
    "Locality is a fundamental feature of the physical world, so it shows up in data drawn from physical observations, like photographs and audio recordings.\n",
    "\n",
    "Locality means most meaningful linear transformations of our input only have large weights in a small number of entries that are close to one another, rather than having equally large weights in all entries.\n",
    "\n",
    "**3. Translation Equivariance:**\n",
    "\n",
    "Another type of structure commonly observed is \"translation equivariance\" -- the top-left pixel position is not, in itself, meaningfully different from the bottom-right position or a position in the middle of the image. Relative relationships matter more than absolute relationships.\n",
    "\n",
    "Translation equivariance arises in images because there is generally no privileged vantage point for taking the image. We could just as easily have taken the image while standing a few feet to the left or right, and all of its contents would shift along with our change in perspective.\n",
    "\n",
    "Translation equivariance means that a linear transformation that is meaningful at one position in our input is likely to be meaningful at all other points. We can learn something about a linear transformation from a datapoint where it is useful in the bottom-left and then apply it to another datapoint where it's useful in the top-right.\n",
    "\n",
    "### Strides\n",
    "\n",
    "Stride refers to the number of pixels by which the filter/kernel is shifted over the input image. When performing convolution, the filter slides over the input image with a certain step size determined by the stride. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means the filter moves two pixels at a time, and so on. Larger strides lead to smaller output feature maps because the filter covers fewer positions.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/strides.png\" height=\"300px\">\n",
    "  <figcaption>Convolution layer with strides=2</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Padding\n",
    "\n",
    "Padding is the process of adding extra border pixels around the input image. This is often done before applying convolution to prevent the spatial dimensions of the feature maps from shrinking too much.\n",
    "\n",
    "There are two types of padding in TensorFlow:\n",
    "\n",
    "- Valid (No padding): In this case, no padding is added to the input image. As a result, the spatial dimensions of the feature maps decrease after convolution.\n",
    "\n",
    "- Same (Zero padding): Here, padding is added in such a way that the output feature maps have the same spatial dimensions as the input image. Typically, zero padding (padding with zeros) is used. This helps in preserving spatial information and enables better performance, especially when multiple layers are stacked.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/padding.png\" height=\"300px\">\n",
    "  <figcaption>Convolution layer \"same\" padding</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Size of layers\n",
    "The size of the next layer would be:\n",
    "\n",
    "$$ n_{\\text{H or W}}^{[l]} = \\lfloor \\frac{n_{\\text{H or W}}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1\\rfloor$$\n",
    "\n",
    "$$A^{[l]} \\implies m \\times n_{\\text{H}}^{[l]} \\times n_{\\text{W}}^{[l]} \\times n_c^{[l]}$$\n",
    "\n",
    "Where:\n",
    "- $m$ is number of samples\n",
    "- $f^{[l]}$ is filter size\n",
    "- $p^{[l]}$ is padding\n",
    "- $s^{[l]}$ is strides\n",
    "- $n_c^{[l]}$ is number of filters\n",
    "\n",
    "Each filter is $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]}$\n",
    "\n",
    "The wights have size $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]} \\times n_c^{[l]}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Their goal is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting). The layer will either take the maximum number in the window, or average the values. The layer does not have any weights to learn.\n",
    "\n",
    "The size of the pooling layer is:\n",
    "\n",
    "$$ n_{\\text{H or W}}^{[l]} = \\lfloor \\frac{n_{\\text{H or W}}^{[l-1]} - f^{[l]}}{s^{[l]}} + 1\\rfloor$$\n",
    "$$A^{[l]} \\implies m \\times n_{\\text{H}}^{[l]} \\times n_{\\text{W}}^{[l]} \\times n_c^{[l]}$$\n",
    "\n",
    "Padding is rarely used with pooling. Here is an example of a max pooling layer:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/max pooling.png\" height=\"300px\">\n",
    "  <figcaption>Max pooling layer (2x2 pooling kernel, stride 2, no padding)</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do CNNs learn?\n",
    "\n",
    "<img src=\"images/net_full_layer_0.png\" height=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architectures\n",
    "\n",
    "Typical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps), thanks to the convolutional layers. At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).\n",
    "\n",
    "<img src=\"images/typical cnn.png\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
