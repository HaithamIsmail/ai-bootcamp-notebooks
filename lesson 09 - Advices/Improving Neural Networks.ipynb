{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Neural Networks Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test Sets\n",
    "\n",
    "We usually split our data into three set, the training set, development (Dev, cross-validation) set, and testing set.\n",
    "\n",
    "-\tTrain models on train set\n",
    "-\tTest these models on Dev set and choose model with best performance\n",
    "-\tEvaluate the best model on the Test set to get an unbiased estimate of how good the model is.\n",
    "\n",
    "Previously machine learning engineers used to split about 10-20% of the data for each of the dev and test sets. In big data era, we do not do that anymore. The development and test sets now represent <1% of the data (when talking about >1M samples), even that would be more than 10K examples. Not having a test set is fine as long as you have a dev set.\n",
    "\n",
    "A rule of thumb is to make sure the Dev & Test sets to come from the same distribution, the training set may be different.\n",
    "\n",
    "For example, we expect our traffic to come from Eastern Europe and the Middle East, while the training data is from Western Europe and US.\n",
    "\n",
    "**Train Data:**\n",
    "1.\tGermany\n",
    "2.\tFrance\n",
    "3.\tUnited Kingdom\n",
    "4.\tSpain\n",
    "5.\tUSA\n",
    "\n",
    "**Dev\\Test Data**\n",
    "Middle East\n",
    "1.\tJordan\n",
    "2.\tLebanon\n",
    "3.\tSyria\n",
    "4.\tIraq\n",
    "Eastern Europe:\n",
    "1.\tRussia\n",
    "2.\tUkraine\n",
    "3.\tPoland\n",
    "4.\tRomania\n",
    "5.\tBelarus\n",
    "6.\tCzech Republic\n",
    "\n",
    "A bad practice here would be to take the data from Middle East as *Dev set*, and the data from Eastern Europe as *Test set*, as they do not come from the same distribution anymore. It would better to shuffle the data then sample randomly into Dev and Test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias/Variance\n",
    "\n",
    "Bias is how much the predicted values differ from the actual values. Bias is the inability of a machine learning model to capture the true relationship between the data variables. **It is caused by the erroneous assumptions that are inherent to the learning algorithm**. For example, in linear regression, the relationship between the X and the Y variable is assumed to be linear, when in reality the relationship may not be perfectly linear.\n",
    "\n",
    "variance refers to how much a model's predictions vary or swing around the true values. If a model has high variance, it means it's sensitive to small changes in the training data, which can cause it to make very different predictions when given slightly different input. For example, if two different engineers overfit a data that are very slightly different. They are going to get completely different results. While if the model generalized to the data, the differences will be very minimal. Overfitting (high variance)  occurs when a model learns to memorize the training data too well, capturing noise and random fluctuations in the data rather than the underlying patterns.\n",
    "\n",
    "\n",
    "<table style=\"background-color: white; color: black;\">\n",
    "<tr>\n",
    "  <th>An underfit model</th>\n",
    "  <th>A good fit, generalized well</th>\n",
    "  <th>Overfit model</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"images/bias - variance/high bias.png\" alt=\"underfit\" />\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "   <img src=\"images/bias - variance/good fit.png\" alt=\"Generalized\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img src=\"images/bias - variance/overfit.png\" alt=\"Overfit\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- The underfit model, cannot predict the training examples well, so we say it has high bias\n",
    "- We say that the second model was able to generalize well to the data, it has small error and was able to understand the pattern the data have\n",
    "- The third model is overfitting the data, it can predict the values of the training data too well, but if we try to predict the price for a new size it would most probably be off by a good margin. For example, after the third point, the model says that the price decreases although the size is increasing which is obviously wrong.\n",
    "\n",
    "That’s how we can visualize bias and variance in 2 dimensions. But larger number of dimensions we are not able to do this visualization, so how do we check for bias and variance in this case?\n",
    "\n",
    "We can do that through the error:\n",
    "\n",
    "Let's reconsider the cat classification problem assuming the best possible accuracy is 100% (0% error):\n",
    "\n",
    "<table style=\"background-color: white; color: black;\">\n",
    "<tr>\n",
    "  <th>y = 1</th>\n",
    "  <th>y = 0</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"images/bias - variance/cat.jpg\" alt=\"cat\" height='200px'/>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "   <img src=\"images/bias - variance/doge.png\" alt=\"dog\" height=\"200px\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"background-color: white; color: black; font-size: 15px;\">\n",
    "<tr>\n",
    "  <th>Error</th>\n",
    "  <th>example 1</th>\n",
    "  <th>example 2</th>\n",
    "  <th>example 3</th>\n",
    "  <th>example 4</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>Train set error</td>\n",
    "  <td>1%</td>\n",
    "  <td>15%</td>\n",
    "  <td>15%</td>\n",
    "  <td>0.5%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>Test set error</td>\n",
    "  <td>11%</td>\n",
    "  <td>16%</td>\n",
    "  <td>30%</td>\n",
    "  <td>1%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>Characteristics</td>\n",
    "  <td>high variance</td>\n",
    "  <td>high bias</td>\n",
    "  <td>high variance and high bias</td>\n",
    "  <td>low bias and low variance</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "1. In the first example, the train error is close to 0, so it is performing well on the training set. However, the accuracy on the test set is 11% which is higher by 10% than that of the training set. So the model is overfitting the training data.\n",
    "2. In example 2, the difference between the training and testing is only 1% so there is low variance. However, the error 15% which is far off from the best possible error so the model is still underfitting (high bias)\n",
    "3. The training error in the third example is high (15%) so it has high bias. Moreover, the difference between the train and test errors is high so it also has high variance. Here the model is overfitting some of the training examples but not all of them, with some more training time, it will completely overfit the training data.\n",
    "4. The 4th example is that of a good fit, the training error is close to the optimal error, and the difference between it and the testing set is very low. So we have low bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Error and Human Level Performance\n",
    "\n",
    "Bayes error is the best error you could get for a problem, you can never surpass bayes optimal error. It is not always 0% because for example images could be blurry or audio clips could be noisy.\n",
    "\n",
    "When building a model, the progress until we reach human level performance is fast but gets slower after that. Usually because human level performance is close to bayes optimal error (humans are good at doing these tasks).\n",
    "\n",
    "Humans are quite good at a lot of tasks. So long as ML is worse than humans, you can:\n",
    "-\tGet labeled data from humans\n",
    "-\tGain insight from manual error analysis: Why did a person get this right?\n",
    "-\tBetter analysis of bias/variance\n",
    "\n",
    "<img src='images/bias - variance/human level performance.png' height='300px'>\n",
    "\n",
    "Better Esimation of Bayes error allows us to know where to focus our efforts at this point in the development process. Because humans are quite good at a lot of tasks, we usually take the human level perforamance as a proxy for Bayes error.\n",
    "\n",
    "Let's take medical image classfication example:\n",
    "\n",
    "Suppose:\n",
    "| Type                   | Error Rate |\n",
    "|------------------------|------------|\n",
    "| Typical human error   | 10%        |\n",
    "| Typical doctor        | 3%         |\n",
    "| Experienced doctor    | 0.7%       |\n",
    "| Team of experienced doctors | 0.5% |\n",
    "\n",
    "**What is the \"human-level\" error in this case?**\n",
    "\n",
    "Since the lowest human system has an error of 0.5% then the human-level would be 0.5%. By definition bayes optimal error is the lowest error possible, then bayes error is estimated to be less than <= 0.5%.\n",
    "\n",
    "Using this estimatotion we can decide problems our model has\n",
    "\n",
    "<img src='images/bias - variance/bias-avoidable bias-variance.png' height='300px'>\n",
    "\n",
    "- Bias is the difference train error\n",
    "- Avoidable error is the differnece between the bayes error estimation, the human-level error, and the train error. It is the maximum error we can reduce\n",
    "- Variance is the difference in the errors of the train and dev sets\n",
    "\n",
    "Let's check the following example:\n",
    "\n",
    "<img src='images/bias - variance/bias-avoidable bias-variance-example.png' height='300px'>\n",
    "\n",
    "1. In the first example, the avoidable bias (7%) is larger than the variance (2%%) so we need to focus on the bias\n",
    "2. In the 2nd example, the variance (2%) is larger than the avoidable bias (0.5%) so we must focus on the variance\n",
    "\n",
    "### Surpassing human-level performance\n",
    "\n",
    "It is possble to surpass human-level performance and it is usually in structured data problems:\n",
    "\n",
    "- Online advertising\n",
    "- Product recommendations\n",
    "- Logistics (predicting transit time)\n",
    "- Loan approvals\n",
    "\n",
    "However, in natural perception (NLP, computer vision, etc...) humans tend to be very good in these tasks and harder to surpass.\n",
    "\n",
    "### Strategy to lower error\n",
    "\n",
    "<table style=\"background-color: white; color: black;\">\n",
    "<tr>\n",
    "  <th>Avoidable Bias</th>\n",
    "  <th>Variance</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "    <ul>\n",
    "        <li>Train bigger model</li>\n",
    "        <li>Train longer</li>\n",
    "        <li>Use better optimization algorithm</li>\n",
    "        <li>Change the NN architecure</li>\n",
    "        <li>Hyperparameters search</li>\n",
    "    </ul>\n",
    "  </td>\n",
    "  <td>\n",
    "   <ul>\n",
    "        <li>Traon on more data</li>\n",
    "        <li>Regularization</li>\n",
    "        <li>Change the NN architecure</li>\n",
    "        <li>Hyperparameters search</li>\n",
    "    </ul>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Chain of assumptions in ML\n",
    "\n",
    "1. Fit training set well on cost function (reach human level performance)\n",
    "2. Fit dev set well on cost function (reduce variance)\n",
    "3. Fit test well on cost function (using bigger dev set)\n",
    "4. Performs well in real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singler number Evaluation metrics\n",
    "\n",
    "It is better to setup a single real number evaluation metric that could tell us if the new thing we just tried is better or worse than the last idea.\n",
    "\n",
    "For example, if we look at both at precision and recall, it would be hard to choose between the 2 networks. That’s why we set up the F1 score which will be a compromise between the 2 metrics and tells us which network is better.\n",
    "\n",
    "Having a well defined dev set + a single number evaluation metric allows us to know which of classifier A or classifier B is better. Thus speeding up the iterative process if improving the machine learning project.\n",
    "\n",
    "How should you pick this metric or model?\n",
    "\n",
    "One example is to take the average of the metric. Suppose you have multiple models that receive data from multiple lcocations, they perform differently on each of these locations so it would be hard to choose th better algorithm depending on the recorded accuracies for each country. To fix that, we could simply take the average of the accuracies accross all locations then decide on the best algorithm.\n",
    "\n",
    "Given this example, where a metric performs differently on different locations:\n",
    "\n",
    "| Algorithms | US  | China | India | Other | Average |\n",
    "|------------|-----|-------|-------|-------|---------|\n",
    "| A          | 3%  | 7%    | 5%    | 9%    | 6%      |\n",
    "| B          | 5%  | 6%    | 5%    | 10%   | 6.50%   |\n",
    "| C          | 2%  | 3%    | 4%    | 5%    | 3.50%   |\n",
    "| D          | 5%  | 8%    | 7%    | 2%    | 5.25%   |\n",
    "| E          | 4%  | 5%    | 2%    | 4%    | 3.75%   |\n",
    "| F          | 7%  | 3%    | 8%    | 12%   | 9.50%   |\n",
    "\n",
    "The best algorithm in this case the best algorithm is `C` as it has an average error *3.5%*.\n",
    "\n",
    "\n",
    "But what if we have multiple metrics to choose from?\n",
    "\n",
    "Let's take an example where there are two required metrics, the accuracy and running time:\n",
    "\n",
    "| Classifier | Accuracy | Running Time |\n",
    "|------------|----------|--------------|\n",
    "| A          | 90%      | 80ms         |\n",
    "| B          | 93%      | 97ms         |\n",
    "| C          | 95%      | 1,000 ms     |\n",
    "\n",
    "Should we choose the one with highest accuracy? or the one with lowest running time?\n",
    "\n",
    "Or we should define a new metric that is the weighted sum of these two: \n",
    "\n",
    "$$ MetricX = Accuracy - 0.5\\times Running Time$$\n",
    "\n",
    "But how to do that in the first place, how would we identify the correct weight for each metric.\n",
    "\n",
    "In this case, you should ask the the customer to specificy an optimizing metric and a satisfying metric. Where you should maximize the optimizing metric and just satisfy the condition of the satisfying metric. Let's take the following:\n",
    "\n",
    "- Accuracy as optimizing metrics\n",
    "- Running time as satisfying metric: running time < 100ms\n",
    "\n",
    "This means we need to select the algorithm with the best accuracy and has a running time lower than 100ms, so from the table, that would be `algorithm B`. \n",
    "\n",
    "Algoirthm C has running time of 1000ms > 100ms so it is out of the selection. Although algorithm A is faster by 17ms than algorithm B, this doesnt matter anymore because they both satisfy the condition (<100ms). But algorithm B has higher accuracy, so it is the best classifier in thise case. \n",
    "\n",
    "Whenever you have more than one metric, we always choose one optimizing metric: `N metrics -> 1 optimizing, N-1 satisfying`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Inputs\n",
    "\n",
    "1. Calculate the mean and the standard deviation of the training data\n",
    "2. Subtract the mean from the samples\n",
    "3. Divide by the standard deviation\n",
    "\n",
    "You will need to save the mean and standard deviation as you will have to use them to normalize the test data and any new points.\n",
    "\n",
    "<table style=\"background-color: white; color: black;\">\n",
    "<tr>\n",
    "  <th>Original Data</th>\n",
    "  <th>subtract the mean</th>\n",
    "  <th>Divide by standard deviation</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"images/normalize/normalize 1.png\" alt=\"underfit\" height='200px'/>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "   <img src=\"images/normalize/normalize 2.png\" alt=\"Generalized\" height='200px'/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img src=\"images/normalize/normalize 3.png\" alt=\"Overfit\" height='200px'>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "**Faster convergence:** Normalizing inputs helps to ensure that the optimization process converges more quickly during training.\n",
    "\n",
    "**Reduced vanishing gradients:** Normalization helps to prevent the vanishing gradient problem, where gradients become very small during backpropagation. This problem can significantly slow down or even prevent the training of deep neural networks. Normalizing inputs can keep gradients within a reasonable range, making training more stable.\n",
    "\n",
    "**Improved generalization:** Normalizing inputs can improve the generalization performance of the network. By scaling the inputs to a similar range, the network becomes less sensitive to variations in the input data, which can help it generalize better to unseen examples.\n",
    "\n",
    "**Mitigation of numerical instability (exploding gradients):** In some cases, large input values can lead to numerical instability issues, such as overflow or underflow in the computations performed by the network. Normalizing inputs can help to mitigate these issues and ensure that the network operates correctly across a wide range of input values.\n",
    "\n",
    "<img src='images/normalize/normalize 4.png' height='300px'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique used in machine learning and specifically in the training of neural networks to prevent overfitting and improve the generalization performance of the model. \n",
    "\n",
    "Regularization methods introduce additional constraints or penalties to the optimization objective function during training, discouraging the model from learning overly complex representations that are sensitive to the training data.\n",
    "\n",
    "**L2 Regularization (Ridge Regression):** Also known as weight decay, L2 regularization adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages the model to prefer smaller weight values, effectively shrinking the weights towards zero. The regularization term is typically controlled by a hyperparameter called the regularization parameter or lambda (λ).\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L} \\left( \\hat{y}^{(i)} - y^{(i)} \\right) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} ||W||_2^2$$\n",
    "\n",
    "\n",
    "**L1 Regularization (Lasso Regression):** Similar to L2 regularization, L1 regularization adds a penalty term to the loss function, but it is proportional to the absolute value of the weights instead of their squares. L1 regularization encourages sparsity in the weight vector, leading to some weights being exactly zero. This can be useful for feature selection and creating more interpretable models.\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L} \\left( \\hat{y}^{(i)} - y^{(i)} \\right) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} ||W||_1$$\n",
    "\n",
    "### Dropout Regularization\n",
    "\n",
    "It involves randomly \"dropping out\" (i.e., setting to zero) a proportion of the neurons in a layer during each training iteration. This means that these neurons do not contribute to the forward pass or the backward pass during training.\n",
    "\n",
    "During training, dropout helps to prevent neurons from co-adapting and relying too much on each other. By randomly dropping out neurons on each epoch, the network is forced to learn more robust features and prevents it from memorizing the training data too closely, thus reducing overfitting.\n",
    "\n",
    "At test time (i.e., when making predictions on new data), dropout is typically turned off, and the full network is used.\n",
    "\n",
    "### Other Regularization Methods\n",
    "- Data Augmentation\n",
    "- Early stopping (it downside is that it affects two optimization tasks (cost function & overfitting) in one task making the things you could try more complex). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialization\n",
    "\n",
    "If we initialize the weights of the neurons in a neural network to zeros, just like logistic regression, all neurons in a given layer would have the same output for the same input. During backpropagation, all weights would receive the same gradient updates, leading to symmetric weight updates. In other words, the neuorns will have the same weights at all stages of the training.\n",
    "\n",
    "Subsequent layers would also learn the same features. As a result, the network would fail to learn meaningful representations of the data, and training would be ineffective.\n",
    "\n",
    "The most common initialization techniques are called `Xavier` and `He`:\n",
    "\n",
    "**1. Xavier Initialization:**\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\\right)$$\n",
    "\n",
    "\n",
    "**2. He Initialization:**\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)$$\n",
    "\n",
    "- $W$ represents the weights being initialized.\n",
    "  \n",
    "- $\\mathcal{N}\\left(\\mu, \\sigma^2 \\right)$ denotes a Gaussian (normal) distribution with mean $\\mu$ and variance $\\sigma^2$\n",
    "  \n",
    "- $\\frac{2}{n_{\\text{in}}}$ is the number of neurons in the layer previous to the one being initialized\n",
    "  \n",
    "- $\\frac{2}{n_{\\text{out}}}$ is the number of neurons in the layer being initialized\n",
    "\n",
    "Depends on what activation function we are using, if we are using **ReLU** activation (which we will) it is better to use **He initialization**. The key is to initialize the weights to values that have a variance 2/n. The reduces the risk of Vanishing/Exploding gradients, and thus speed up networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "\n",
    "<table style=\"background-color: white; color: black;\">\n",
    "<tr>\n",
    "  <th>Stochastic GD</th>\n",
    "  <th>Mini-batch GD</th>\n",
    "  <th>Batch GD</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "  Update weights on each training sample\n",
    "  </td>\n",
    "  <td>\n",
    "  Take small batches of data (not too big, not too small). Update the weights after calculating the loss for this mini-batch\n",
    "  </td>\n",
    "  <td>\n",
    "  Update the weights after calculating the loss for all the samples\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>\n",
    "  Loses speed up from vectorization\n",
    "  </td>\n",
    "  <td>\n",
    "  <ul>\n",
    "    <li>Fastest Learning</li>\n",
    "    <li>Uses vectorization</li>\n",
    "    <li>Make progress without waiting to process the whole dataset</li>\n",
    "  </ul>\n",
    "  </td>\n",
    "  <td>\n",
    "  Takes too long to finish each iteration\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<img src='images/mini batch/Mini Batch Gradient Descent.png' height='300px' width='800px'>\n",
    "<img src='images/mini batch/Mini Batch Gradient Descent 2.png' height='300px' width='500px'>\n",
    "\n",
    "If we have a small dataset (m <= 2000) we use batch gradient descent. Otherwise, use mini-batch gradient descent. Usually we use a power of 2, but you need to make sure that the mini-batch fits in your CPU/GPU memory:\n",
    "\n",
    "$$ \\text{Mini-batch size: }64, \\ 128, \\ 256, \\ 512, \\text{and in rare cases} \\ 1024$$\n",
    "\n",
    "### Gradient Descent with momentum\n",
    "\n",
    "This is an update version of gradient descent that aims to damp the oscillations. It uses a concept called exponentially weighted averages, that will help average the oscillations to 0 and keep the descent in the best direction.\n",
    "\n",
    "$$ v_{dw} = \\beta v_{dw} + (1 - \\beta)dW$$\n",
    "$$ v_{db} = \\beta v_{db} + (1 - \\beta)db$$\n",
    "\n",
    "$$ \\implies W = W - \\alpha v_{dw}, \\ b = b - \\alpha v_{db} $$\n",
    "\n",
    "\n",
    "\n",
    "<img src='images/mini batch/momentum.jpeg' height='300px' width='500px'>\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "RMSprop (Root Mean Square Propagation) is an optimization algorithm that is rarely used anymore. It is designed to address some limitations of traditional gradient descent algorithms, such as the sensitivity to the choice of learning rate and the inability to efficiently handle sparse gradients, and to damp the oscillations during the updates.\n",
    "\n",
    "$$ s_{dw} = \\beta s_{dw} + (1 - \\beta)dW^2$$\n",
    "$$ s_{db} = \\beta s_{db} + (1 - \\beta)db^2$$\n",
    "\n",
    "$$ \\implies W = W - \\alpha \\frac{dw}{\\sqrt{s_{db}}}$$\n",
    "$$b = b - \\alpha \\frac{dw}{\\sqrt{s_{db}}} $$\n",
    "\n",
    "### Adam\n",
    "\n",
    "Adam (short for Adaptive Moment Estimation) is an optimization algorithm commonly used for training deep neural networks. It combines the concepts of momentum and adaptive learning rates to achieve faster convergence and better generalization performance. It is a combination of concepts from RMSprop and gradient descent with momentum\n",
    "\n",
    "$$ v_{dw} = \\beta_1 v_{dw} + (1 - \\beta_1)dW$$\n",
    "$$ v_{db} = \\beta_1 v_{db} + (1 - \\beta_1)db$$\n",
    "$$ s_{dw} = \\beta_2 s_{dw} + (1 - \\beta_2)dW^2$$\n",
    "$$ s_{db} = \\beta_2 s_{db} + (1 - \\beta_2)db^2$$\n",
    "$$ v_{dw}^{\\text{corrected}} = \\frac{v_{dw}}{(1-\\beta_1^t)}, \\ v_{db}^{\\text{corrected}} = \\frac{v_{db}}{(1-\\beta_1^t)}$$\n",
    "$$ s_{dw}^{\\text{corrected}} = \\frac{s_{dw}}{(1-\\beta_2^t)}, \\ s_{db}^{\\text{corrected}} = \\frac{s_{db}}{(1-\\beta_2^t)}$$\n",
    "\n",
    "$$ \\implies W = W - \\alpha \\frac{v_{dw}^{\\text{corrected}}}{\\sqrt{s_{dw}^{\\text{corrected}}} + \\epsilon}, \\ b = b - \\alpha \\frac{v_{db}^{\\text{corrected}}}{\\sqrt{s_{db}^{\\text{corrected}}} + \\epsilon}$$\n",
    "\n",
    "$\\text{Where t is the number of current iteration}$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
