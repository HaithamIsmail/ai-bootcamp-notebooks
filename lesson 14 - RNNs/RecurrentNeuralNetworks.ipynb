{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "A recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data. These deep learning algorithms are commonly used for ordinal or temporal problems, such as language translation, natural language processing (nlp), speech recognition, and image captioning; they are incorporated into popular applications such as Siri, voice search, and Google Translate.\n",
    "\n",
    "RNNs are distinguished by their “memory” that enables it to factor previous input when producing output. The short-term memory allows the network to retain past information and, hence, uncover relationships between data points that are far from each other. \n",
    "\n",
    "<img src=\"images/applications.png\"  width=\"500px\">\n",
    "\n",
    "**So why not use standard networks?**\n",
    "\n",
    "In standard networks:\n",
    "\n",
    "-\tInputs and outputs cannot be different lengths in different examples\n",
    "-\tDoesn’t share features learned across different positions in the text\n",
    "-\tIt requires much larger number of parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics fo RNNs\n",
    "The main characteristic of RNNs, is that input and output do need to have fixed length. Now, let’s take an idiom, such as “feeling under the weather”, which is commonly used when someone is ill, to aid us in the explanation of RNNs. In order for the idiom to make sense, it needs to be expressed in that specific order. As a result, recurrent networks need to account for the position of each word in the idiom and they use that information to predict the next word in the sequence.\n",
    "\n",
    "Another distinguishing characteristic of recurrent networks is that they share parameters across each layer of the network. While feedforward networks have different weights across each node, recurrent neural networks share the same weight parameter within each layer of the network.\n",
    "\n",
    "They leverage backpropagation through time (BPTT) algorithm to determine the gradients, which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. These calculations allow us to adjust and fit the parameters of the model appropriately. BPTT differs from the traditional approach in that BPTT sums errors at each time step whereas feedforward networks do not need to sum errors as they do not share parameters across each layer\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"background-color: white;\" width=\"500px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "The notation for an input sequence $x$ of length $T_x$ or an output sequence $y$ of length $Ty$ is as follows (note the new notation with chevrons around the indices to enumerate the tokens):\n",
    "\n",
    "$$ x=x^{<1>},x^{<2>},...,x^{<t>},...,x^{<Tx>} $$\n",
    "$$ y=y^{<1>},y^{<2>},...,y^{<t>},...,y^{<Ty>} $$\n",
    "\n",
    "where:\n",
    "- $T_x^{(i)}$: length of input of ith example\n",
    "- $T_y^{(i)}$: length if output of ith example\n",
    "- $X^{(i)<t>}$:  t-th element of ith example\n",
    "- $Y^{(i)<t>}$: t-th element of ith output\n",
    "\n",
    "\n",
    "The input and the output sequence don’t need to be of the same length $(T^{(i)}_x \\neq T^{(i)}_y)$. Also the length of the individual training samples can vary $(T^{(i)}_y \\neq T^{(j)}_y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Unidirectional RNN\n",
    "\n",
    "A simple RNN only has one layer through which the tokens pass during training/processing. However, the result of this processing has an influence on the processing of the next token. Consider the following sample architecture of a simple RNN:\n",
    "\n",
    "<img src=\"images/example rnn.png\">\n",
    "\n",
    "The RNN processes each token $x^{<t>}$ individually from left to right, one after the other. In each step $t$ the RNN tries to predict the output $\\hat{y}^{<t>}$ from the input token $x^{<t>}$ and the previous activation $a^{<t−1>}$. To determine the influence of the activation and the input token and the two weight matrices $W_{aa}$ and $W_{ax}$ are used. There is also a matrix $W_{ya}$ that governs the output predictions.\n",
    "\n",
    "Those matrices are the same for each step, i.e. they are shared for a single training instance. This way the layer is recursively used to process the sequence. A single input token can therefore not only directly influence the output at a given time step, but also indirectly the output of subsequent steps (thus the term recurrent). Vice versa a single prediction at time step $<t>$ not only depends on a single input token, but on several previously seen tokens \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "The activation $a^{<t>}$ and prediction $ \\hat{y} ^{<t>} $ for a single time step $t$ can be calculated as follows (for the first token the zero vector is often used as the previous activation):\n",
    "\n",
    "$$ a^{<t>} = g_1(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) $$\n",
    "\n",
    "$$ \\hat{y}^{<t>} = g_2(W_{ya}a^{<t>} + b_y) $$\n",
    "\n",
    "\n",
    "the activation functions $g_1$ and $g_2$ can be different. The activation function to calaculate the next activation $(g_1)$ is often **Tanh** or **ReLU**. The activation function to predict the next output $(g_2)$ is often the **Sigmoid** function for binary classification or else **Softmax**. The notation of the weight matrices is by convention so that the first index denotes the output quantity and the second index the input quantity. $W_{ax}$ for example means “use the weights in $W$ to compute some output $a$ from input $x$”.\n",
    "\n",
    "This calculation can further be simplified by concatenating the matrices $W_{aa}$ and $W_{ax}$ into a single matrix $W_a$ and stacking:\n",
    "\n",
    "$$\n",
    "Wa = \\begin{bmatrix} W_{aa} \\ | \\ W_{ax} \\end{bmatrix} \\\\[10pt]\n",
    "\\begin{bmatrix} a^{<t-1>}, \\ x^{<t>} \\end{bmatrix} =  \n",
    "\\begin{bmatrix} a^{<t-1>} \\\\ x^{<t>} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The simplified formula to calculate forward propagation is then:\n",
    "\n",
    "$$ a^{<t>} = g_1(W_{a} \\begin{bmatrix} a^{<t-1>}, \\ x^{<t>} \\end{bmatrix} + b_a) $$\n",
    "\n",
    "$$ \\hat{y}^{<t>} = g_2(W_{y}a^{<t>} + b_y) $$\n",
    "\n",
    "### Backpropagation Through Time\n",
    "\n",
    "Because the input is read sequentially and the RNN computes a prediction in each step, the output is a sequence of predictions. The loss function for backprop for a single time step <t> in binary classfication could be:\n",
    "\n",
    "$$ L^{<t>}(\\hat{y}^{<t>},y^{<t>}) = −y^{<t>}log(\\hat{y}^{<t>})−(1−y^{<t>})log(1−\\hat{y}^{<t>}) $$\n",
    "\n",
    "The formula to compute the overall cost for a sequence of $T_x$ predictions is therefore:\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y},y) =  \\sum_{t=1}^{Ty}\\mathcal{L}^{<t>}(\\hat{y}^{<t>},y^{<t>}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of RNNs\n",
    "\n",
    "The four commonly used types of Recurrent Neural Networks are:\n",
    "\n",
    "**1. One-to-One**\n",
    "\n",
    "The most straightforward type of RNN is One-to-One, which allows a single input and a single output. It has fixed input and output sizes and acts as a standard neural network. The One-to-One application can be found in Image Classification.\n",
    "\n",
    "<img src=\"images/one to one.png\" width=\"500px\">\n",
    "\n",
    "**2. One-to-Many**\n",
    "\n",
    "One-to-Many is a type of RNN that expects multiple outputs on a single input given to the model. The input size is fixed and gives a series of data outputs. Its applications can be found in applications like Music Generation and Image Captioning.\n",
    "\n",
    "<img src=\"images/one to many.png\" width=\"500px\">\n",
    "\n",
    "**3. Many-to-one**\n",
    "\n",
    "Many-to-One RNN converges a sequence of inputs into a single output by a series of hidden layers learning the features. Sentiment Analysis is a common example of this type of Recurrent Neural Network.\n",
    "\n",
    "<img src=\"images/many to one.png\" width=\"500px\">\n",
    "\n",
    "**4. Many-to-many**\n",
    "\n",
    "Many-to-Many is used to generate a sequence of output data from a sequence of input units. It is further divided into ﻿the following two subcategories:\n",
    "\n",
    "- Equal Size: In this case, the input and output layer size is exactly the same, such as voice-to-text conversion.\n",
    "  \n",
    "<img src=\"images/many to many 1.png\" width=\"500px\">\n",
    "\n",
    "- Unequal Size: In this case, inputs and outputs have different numbers of units. Its application can be found in Machine Translation.\n",
    "  \n",
    "<img src=\"images/many to many 2.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of RNN\n",
    "\n",
    "Simple RNN models usually run into two major issues. These issues are related to gradient, which is the slope of the loss function along with the error function.\n",
    "\n",
    "- **Vanishing Gradient**: Long sentences might have dependencies between it start and end. However, with many steps the gradients might become exponentially long, also known as vanishing gradients. (when the gradient becomes so small that updating parameters becomes insignificant; eventually the algorithm stops learning.)\n",
    "  \n",
    "- **Exploding Gradient** problem occurs when the gradient becomes too large, which makes the model unstable. In this case, larger error gradients accumulate, and the model weights become too large. This issue can cause longer training times and poor model performance.\n",
    "\n",
    "The simple solution to these issues is to reduce the number of hidden layers within the neural network, which will reduce some complexity in RNNs. These issues can also be solved by using advanced RNN architectures such as LSTM and GRU.\n",
    "\n",
    "## Advanced RNN Architectures\n",
    "\n",
    "The simple RNN repeating modules have a basic structure with a single tanh layer. RNN simple structure suffers from short memory, where it struggles to retain previous time step information in larger sequential data. These problems can easily be solved by **long short term memory (LSTM)** and **gated recurrent unit (GRU)**, as they are capable of remembering long periods of information.\n",
    "\n",
    "### Gated Recurrent Units\n",
    "\n",
    "**Gated Recurrent Units (GRU)** are a modification for the hidden layeres in an RNN that help mitigating the problem of vanishing gradients. GRU are cells in a RNN that have a memory which serves as an additional input to make a prediction. To better understand how GRU cells work, consider the following image depicting how a normal RNN cell works:\n",
    "\n",
    "<img src=\"images/rnn-cell.png\">\n",
    "\n",
    "#### Simple GRU\n",
    "\n",
    "The following picture illustrates the calculations inside a GRU cell:\n",
    "\n",
    "<img src=\"images/gru-cell.png\">\n",
    "\n",
    "GRU units have a memory cell $c^{<t>}$ to “remember”. Note that for GRU cells $c^{<t>}= a^{<t>}$ but we still use the variable $c$ for consistency reasons, we use the same symbol. In each time step a value $\\tilde{c}$ is calculated as a candidate to replace the existing content of the memory cell $c$. This candidate uses an activation function (e.g. tanh), its own trainable parameter matrix $W_c$ and a separate bias $b_c$.\n",
    "\n",
    "$$\n",
    "\\tilde{c}^{<t>} = \\tanh \\left( W_c \\left[ c^{<t-1>}, x^{<t>} \\right]  + b_c \\right)\n",
    "$$\n",
    "\n",
    "After calculating the candidate $\\tilde{c}^{<t>}$ we use an update-gate $Γ_u$ to decide whether we should update the cell with this value or keep the old value. The value for $\\Gamma_u$ can be calculated using another trainable parameter matrix $W_u$ and bias $b_u$. Because Sigmoid is used as the activation function, the values for $\\Gamma_u$ are always between 0 and 1 (for simplification you can also think of $\\Gamma_u$ to be either exactly 0 or exactly 1).\n",
    "\n",
    "$$\n",
    "\\Gamma_u = \\sigma\\left(  W_u \\left[ c^{<t-1>, x^{<t>}} \\right] + b_u \\right)\n",
    "$$\n",
    "\n",
    "This gate is the key component of a GRU because it “decides” when to update the memory cell. Combining previous two equations gives us the following formula to calculate the value of the memory cell in each time step:\n",
    "\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}\n",
    "$$\n",
    "\n",
    "The black box in the figure refers to this equation\n",
    "\n",
    "#### Full GRU\n",
    "\n",
    "The above explanations described a simplified version of a GRU with only one gate $\\Gamma_u$ to decide whether to update the cell value or not. Full GRUs however usually have an additional parameter $\\Gamma_r$ that describes the relevance of individual features, which again uses its own parameter matrix $W_r$ and bias $b_r$ to be trained:\n",
    "\n",
    "$$ \\Gamma_r = \\sigma\\left(  W_r \\left[ c^{<t-1>, x^{<t>}} \\right] + b_r \\right) $$\n",
    "\n",
    "<img src=\"images/gru table.png\" width=\"500px\">\n",
    "\n",
    "### Long Short Term Memory\n",
    "\n",
    "An advanced alternative to GRU are Long Short Term Memory (LSTM) cells. LSTM cells can be considered a more general and more powerful version of GRU cells. Such cells also use a memory cell $c$ to remember something. However, the update of this cell is slightly different from GRU cells.\n",
    "\n",
    "In contrast to GRU cells, the memory cell does not correspond to the activation value anymore, so for LSTM-cells c<t>≠a<t>. It also does not use a relevance gate $\\Gamma_r$ anymore but rather a forget-gate $\\Gamma_f$ that governs whether to forget the current cell value or not. Finally, there is a third parameter $\\Gamma_o$ to act as output-gate and is used to scale the update memory cell value to calculate the activation value for the next iteration.\n",
    "\n",
    "<img src=\"images/lstm-cell 2.png\" width=\"500px\">\n",
    "\n",
    "<img src=\"images/lstm table.png\" width=\"500px\">\n",
    "\n",
    "### GRU vs. LSTM\n",
    "\n",
    "There is not an universal rule when to use GRU- or LSTM-cells. GRU cells represent a simpler model, hence they are more suitable to build a bigger RNN model because they are computationally more efficient and the RNN will scale faster. On the other hand the LSTM-cells are more powerful and more flexible, but they also require more training data. In case of doubt, try LSTM cells because they have sort of become state of the art for RNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs\n",
    "\n",
    "Unidirectional RNNs only consider already seen tokens at a time step $<t>$ to make a prediction. In contrast, bidirectional RNN (BRNN) also take subsequent (future) tokens into account. This is for example helpful for named entity recognition (NER) when trying to predict whether the word `Teddy` is part of a name in the following two sencences:\n",
    "\n",
    "`<he> <said> <teddy> <bears> <are> <on> <sale> <EOS>`\n",
    "\n",
    "`<he> <said> <teddy> <roosevelt> <was> <a> <great> <president> <EOS>`\n",
    "\n",
    "Just by looking at the previously seen words it is not clear at time step $t=3$ whether $<teddy>$ is part of a name or not. To do that we need the information of the following tokens. A BRNN can do this using an additional layer. During forward propagation the activation values $\\stackrel{\\rightarrow}{a}$ are computed as seen above from the input tokens and the previous activation values using an RNN cell (normal RNN cell, GRU or LSTM). The second part of forward propagation calculates the values $\\stackrel{\\leftarrow}{a}$ from left to right using the additional layer. The following picture illustrates this. Note that the arrows in blue and green only indicate the order in which the tokens are evaluated. It does not indicate backpropagation.\n",
    "\n",
    "<img src=\"images/brnn.png\">\n",
    "\n",
    "After a single pass of forward propagation a prediction at time step $t$ can be made by stacking the activations of both directions and calculating the prediction value as follows:\n",
    "\n",
    "$$ \\hat{y}^{<t>} = g \\left( W_y \\left[ \\overrightarrow{a}^{<t>}, \\overleftarrow{a}^{<t>} \\right] + b_y \\right) $$\n",
    "\n",
    "The advantage of BRNN is that it allows to take into account words from both directions when making a prediction, which makes it a good fit for many language-related applications like machine translation. On the downside, because tokens from both directions are considered, the whole sequence needs to be processed before a prediction can be made. This makes it unsuitable for tasks like real-time speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "\n",
    "The RNNs we have seen so far consisted actually of only one layer (with the exception of the BRNN which used an additional layer for the reverse direction). We can however stack several of those layers on top of each other to get a Deep RNN. In such a network, the results from one layer are passed on to the next layer in each time step $t$:\n",
    "\n",
    "<img src=\"images/deep-rnn.png\">\n",
    "\n",
    "The activation $a^{[l]<t>}$ for layer $l$ at time step $t$ can be calculated as follows:\n",
    "\n",
    "$$ a^{[l]<t>} = g\\left( W_a^{[l]} \\left[ a^{[l]<t-1>}, a^{[l-1]<t>} \\right] + b_a^{[l]}\\right) $$\n",
    "\n",
    "Deep-RNN can become computationally very expensive quickly, therefore they usually do not contain as many stacked layers as we would expect in a conventional Deep-NN."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
